{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MAgent Independent.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOP-_mwzy4t0"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import statistics\n",
        "import matplotlib.pyplot as plt\n",
        "# import pygame as pg\n",
        "import random\n",
        "import math\n",
        "import glob\n",
        "import os\n",
        "from PIL import Image\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFMpcfkWy4t_"
      },
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "    def __init__(self, num_states, hidden_units, num_actions):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.input_layer = tf.keras.layers.InputLayer(input_shape=(num_states,))\n",
        "        self.hidden_layers = []\n",
        "        for i in hidden_units:\n",
        "            self.hidden_layers.append(tf.keras.layers.Dense(\n",
        "                i, activation='relu', kernel_initializer='RandomNormal'))\n",
        "        self.output_layer = tf.keras.layers.Dense(\n",
        "            num_actions, activation='linear', kernel_initializer='RandomNormal')\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, inputs):\n",
        "        z = self.input_layer(inputs)\n",
        "        for layer in self.hidden_layers:\n",
        "            z = layer(z)\n",
        "        output = self.output_layer(z)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCG_lBOxy4uD"
      },
      "source": [
        "class DQN:\n",
        "    def __init__(self, num_states, num_actions, hidden_units, gamma, max_experiences, min_experiences, batch_size, lr):\n",
        "        self.num_actions = num_actions\n",
        "        self.batch_size = batch_size\n",
        "        self.optimizer = tf.optimizers.Adam(lr)\n",
        "        self.gamma = gamma\n",
        "        self.model = MyModel(num_states, hidden_units, num_actions)\n",
        "        self.experience = {'s': [], 'a': [], 'r': [], 's2': [], 'done': []}\n",
        "        self.max_experiences = max_experiences\n",
        "        self.min_experiences = min_experiences\n",
        "        self.iter = 0\n",
        "        \n",
        "    def predict(self, inputs):\n",
        "        return self.model(np.atleast_2d(inputs.astype('float32')))\n",
        "    \n",
        "    def train(self, TargetNet):\n",
        "        if len(self.experience['s']) < self.min_experiences:\n",
        "            return 0\n",
        "        ids = np.random.randint(low=0, high=len(self.experience['s']), size=self.batch_size)\n",
        "        states = np.asarray([self.experience['s'][i] for i in ids])\n",
        "        actions = np.asarray([self.experience['a'][i] for i in ids])\n",
        "        rewards = np.asarray([self.experience['r'][i] for i in ids])\n",
        "        states_next = np.asarray([self.experience['s2'][i] for i in ids])\n",
        "        dones = np.asarray([self.experience['done'][i] for i in ids])\n",
        "        value_next = np.max(TargetNet.predict(states_next), axis=1)\n",
        "        actual_values = np.where(dones, rewards, rewards+self.gamma*value_next)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            selected_action_values = tf.math.reduce_sum(\n",
        "                self.predict(states) * tf.one_hot(actions, self.num_actions), axis=1)\n",
        "            loss = tf.math.reduce_mean(tf.square(actual_values - selected_action_values))\n",
        "        variables = self.model.trainable_variables\n",
        "        gradients = tape.gradient(loss, variables)\n",
        "        self.optimizer.apply_gradients(zip(gradients, variables))\n",
        "        self.iter += 1\n",
        "        return loss\n",
        "    \n",
        "    def get_action(self, NN_input, epsilon):\n",
        "        if random.uniform(0, 1) < epsilon:\n",
        "            return np.random.choice(self.num_actions)\n",
        "        else:\n",
        "            return np.argmax(self.predict(np.atleast_2d(NN_input))[0])\n",
        "        \n",
        "    def add_experience(self, exp):\n",
        "        if len(self.experience['s']) >= self.max_experiences:\n",
        "            for key in self.experience.keys():\n",
        "                self.experience[key].pop(0)\n",
        "        for key, value in exp.items():\n",
        "            self.experience[key].append(value)\n",
        "\n",
        "    def copy_weights(self, TrainNet):\n",
        "        variables1 = self.model.trainable_variables\n",
        "        variables2 = TrainNet.model.trainable_variables\n",
        "        for v1, v2 in zip(variables1, variables2):\n",
        "            v1.assign(v2.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9o-z-9oy4vP"
      },
      "source": [
        "class Agent():\n",
        "    def __init__(self, isPursuer, position, TargetNet, TrainNet):\n",
        "        if isPursuer == 'p':\n",
        "          self.isPursuer = 1\n",
        "        else:\n",
        "          self.isPursuer = 0  \n",
        "        self.position = position\n",
        "        self.TrainNet = None\n",
        "        self.TargetNet = None\n",
        "        self.rewards = []\n",
        "        self.currReward = 0\n",
        "        self.losses = []\n",
        "        self.currLosses = []\n",
        "        self.lastAction = 0\n",
        "        self.num_agents_in_radius = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EAzJSp-czuI"
      },
      "source": [
        "class World():\n",
        "    def __init__(self, numPursuers, numEvaders):\n",
        "        self.numPursuers = numPursuers\n",
        "        self.numEvaders = numEvaders\n",
        "        self.pursuers = []\n",
        "        self.evaders = []\n",
        "        self.radius = 20\n",
        "        self.action_space_e = [np.array([0,2]),np.array([2,0]),np.array([0,-2]), np.array([-2,0]), np.array([2,2]), np.array([2,-2]), np.array([-2,2]), np.array([-2,-2]), np.array([0,1]),np.array([1,0]),np.array([0,-1]), np.array([-1,0]), np.array([1,1]), np.array([1,-1]), np.array([-1,1]), np.array([-1,-1])]\n",
        "        # Assumption : Pursuer action space starts with (0,0)\n",
        "        self.action_space_p = [np.array([0,0]), np.array([0,1]),np.array([1,0]),np.array([0,-1]), np.array([-1,0])]\n",
        "\n",
        "        # self.action_space_e = [np.array([0,1]),np.array([1,0]),np.array([0,-1]), np.array([-1,0])]        \n",
        "\n",
        "    def initialize_pursuers(self):\n",
        "        for i in range(self.numPursuers):\n",
        "            position_list = self.return_pursuer_positions(self.numPursuers)\n",
        "            pos_i = position_list[i]\n",
        "            self.pursuers.append(Agent('p', pos_i, None, None)) \n",
        "\n",
        "    def initialize_evaders(self):\n",
        "        self.evaders.append(Agent('e', np.array((10,10)), None, None))   \n",
        "\n",
        "    def reset_world(self):\n",
        "        position_list = self.return_pursuer_positions(self.numPursuers)\n",
        "        for i in range(self.numPursuers):\n",
        "            self.pursuers[i].position = position_list[i]\n",
        "            if len(self.pursuers[i].currLosses)!=0:\n",
        "                self.pursuers[i].rewards.append(self.pursuers[i].currReward)\n",
        "                self.pursuers[i].currReward = 0\n",
        "                self.pursuers[i].losses.append(statistics.mean(self.pursuers[i].currLosses))\n",
        "                self.pursuers[i].currLosses = []\n",
        "\n",
        "        for i in range(self.numEvaders):\n",
        "            self.evaders[i].position = np.array((random.randint(0,19),random.randint(0,19)))\n",
        "            if len(self.evaders[i].currLosses)!=0:\n",
        "                self.evaders[i].rewards.append(self.evaders[i].currReward)\n",
        "                self.evaders[i].currReward = 0\n",
        "                self.evaders[i].losses.append(statistics.mean(self.evaders[i].currLosses))\n",
        "                self.evaders[i].currLosses = []        \n",
        "\n",
        "\n",
        "    def return_pursuer_positions(self, numPursuers):\n",
        "        position_list = [np.array((random.randint(0,19),random.randint(0,19))) for i in range(self.numPursuers)]\n",
        "        return position_list\n",
        "\n",
        "    def get_NN_input(self, state, i, char):\n",
        "      if char == 'p':\n",
        "        n = len(state)\n",
        "        # nearby_agents_actions = []\n",
        "        # for agent in self.pursuers:\n",
        "        #     temp_dist = dist = np.linalg.norm(self.pursuers[i].position - agent.position)\n",
        "        #     if temp_dist < self.radius:\n",
        "        #         nearby_agents_actions.append(self.action_space_p[agent.lastAction])\n",
        "        #     agent.num_agents_in_radius()\n",
        "        \n",
        "        # nearby_agents_actions = np.array(nearby_agents_actions)\n",
        "        # mean_of_actions = np.mean(nearby_agents_actions, axis = 0)        \n",
        "        nn_input = np.append(state[2*i:2*(i+1)],state[n-2:n])\n",
        "        return nn_input\n",
        "      else:\n",
        "          return np.append(self.evaders[0].position,self.pursuers[0].position)\n",
        "      \n",
        "\n",
        "    def initialize_NN(self):\n",
        "        gamma = 0.99\n",
        "        copy_step = 1000\n",
        "        # Full state space assumption\n",
        "        num_inputs_p = 4 #mean field approximation\n",
        "        num_inputs_e = (self.numPursuers + self.numEvaders)*2\n",
        "        num_actions_p = len(self.action_space_p)\n",
        "        num_actions_e = len(self.action_space_e)\n",
        "        hidden_units_p = [100, 100]\n",
        "        hidden_units_e = [100, 100]\n",
        "        max_experiences = 100000\n",
        "        min_experiences = 512\n",
        "        batch_size = 512\n",
        "        lr_p = 1e-4\n",
        "        lr_e = 1e-4\n",
        "\n",
        "        for i in range(len(self.pursuers)):\n",
        "            TrainNet_p = DQN(num_inputs_p, num_actions_p, hidden_units_p, gamma, max_experiences, min_experiences, batch_size, lr_p)\n",
        "            TargetNet_p = DQN(num_inputs_p, num_actions_p, hidden_units_p, gamma, max_experiences, min_experiences, batch_size, lr_p)\n",
        "            # TrainNet_p.model = tf.keras.models.load_model('Train_smart_pursuer1')\n",
        "            # TargetNet_p.model = tf.keras.models.load_model('Target_smart_pursuer1')\n",
        "            self.pursuers[i].TrainNet = TrainNet_p\n",
        "            self.pursuers[i].TargetNet = TargetNet_p\n",
        "\n",
        "        for i in range(len(self.evaders)):\n",
        "              TrainNet_e = DQN(num_inputs_e, num_actions_e, hidden_units_e, gamma, max_experiences, min_experiences, batch_size, lr_e)\n",
        "              TargetNet_e = DQN(num_inputs_e, num_actions_e, hidden_units_e, gamma, max_experiences, min_experiences, batch_size, lr_e)\n",
        "              # TrainNet_p.model = tf.keras.models.load_model('Train_smart_pursuer1')\n",
        "              # TargetNet_p.model = tf.keras.models.load_model('Target_smart_pursuer1')\n",
        "              self.evaders[i].TrainNet = TrainNet_e\n",
        "              self.evaders[i].TargetNet = TargetNet_e\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbGgoQSwy4uL"
      },
      "source": [
        "class MarlEnv():\n",
        "\n",
        "    def __init__(self):\n",
        "        self.dir = 1\n",
        "        self.numPursuers = 4\n",
        "        self.numEvaders = 1\n",
        "        self.world = World(self.numPursuers, self.numEvaders)\n",
        "        self.world.initialize_pursuers()\n",
        "        self.world.initialize_evaders()\n",
        "        self.world.initialize_NN()        \n",
        "        self.iter = 0        \n",
        "#         self.grid = grid\n",
        "        print(\"Init\")\n",
        "\n",
        "    def take_action_p(self, i, action):\n",
        "        self.world.pursuers[i].position += action\n",
        "\n",
        "    def take_action_e(self, i, action):\n",
        "        self.world.evaders[i].position += action\n",
        "                                    \n",
        "#         if (self.evader_p[0]==5 and self.dir==-1) or (self.evader_p[0]==15 and self.dir==1):\n",
        "#             self.dir = (-1)*self.dir\n",
        "\n",
        "#         if self.dir == 1:\n",
        "#             self.evader_p[0] += 1\n",
        "#         else:\n",
        "#             self.evader_p[0] -= 1\n",
        "           \n",
        "        \n",
        "    def step(self, actions_p, actions_e, state):\n",
        "        violated_p =[]\n",
        "        violated_e =[]\n",
        "        dones = []\n",
        "        rewards_p = []\n",
        "        rewards_e = []\n",
        "\n",
        "        for i in range(self.numEvaders):\n",
        "            self.take_action_e(i, self.world.action_space_e[actions_e[i]]) \n",
        "\n",
        "        for i in range(self.numPursuers):\n",
        "            self.take_action_p(i, self.world.action_space_p[actions_p[i]])\n",
        "                                               \n",
        "        self.iter += 1\n",
        "        next_state = []\n",
        "        for agent in self.world.pursuers:\n",
        "            next_state.append(agent.position[0])\n",
        "            next_state.append(agent.position[1])\n",
        "\n",
        "        for agent in self.world.evaders:\n",
        "            next_state.append(agent.position[0])\n",
        "            next_state.append(agent.position[1])  \n",
        "\n",
        "\n",
        "        for i in range(self.numPursuers):\n",
        "            reward, done, violated = self.compute_reward_p(i, next_state)\n",
        "            self.world.pursuers[i].currReward += reward\n",
        "            if violated:\n",
        "                self.take_action_p(i, (-1)*self.world.action_space_p[actions_p[i]])\n",
        "            rewards_p.append(reward)\n",
        "            violated_p.append(violated)\n",
        "            dones.append(done)\n",
        "\n",
        "        for i in range(self.numEvaders):\n",
        "            reward, done, violated = self.compute_reward_e(i, next_state)\n",
        "            self.world.evaders[i].currReward += reward\n",
        "            if violated:\n",
        "                self.take_action_e(i, (-1)*self.world.action_space_e[actions_e[i]])\n",
        "            rewards_e.append(reward)\n",
        "            violated_e.append(violated)\n",
        "            dones.append(done)   \n",
        "\n",
        "        next_state = []\n",
        "        for agent in self.world.pursuers:\n",
        "            next_state.append(agent.position[0])\n",
        "            next_state.append(agent.position[1])\n",
        "\n",
        "        for agent in self.world.evaders:\n",
        "            next_state.append(agent.position[0])\n",
        "            next_state.append(agent.position[1])                      \n",
        "            \n",
        "        return next_state, rewards_p, rewards_e, any(dones), violated_p, violated_e\n",
        "\n",
        "    def compute_reward_p(self, i, state):\n",
        "        pursuer_pos = state[2*i: (2*i)+2]\n",
        "        done = False\n",
        "        violated = False\n",
        "        if state[2*i] <0 or state[(2*i)+1]>19 or state[2*i] > 19 or state[(2*i)+1] < 0:\n",
        "            violated = True\n",
        "            return -5, False, True\n",
        "        #one evader assumption\n",
        "      \n",
        "        elif np.array_equal(pursuer_pos, self.world.evaders[0].position):\n",
        "            return 10, True, False\n",
        "\n",
        "        dist = np.linalg.norm(pursuer_pos - self.world.evaders[0].position)\n",
        "#         dist = abs(state[0] - state[2]) + abs(state[1] - state[3])\n",
        "        dist = dist/(20*math.sqrt(2))\n",
        "        return -dist, False, False\n",
        "#         return -1\n",
        "\n",
        "    def compute_reward_e(self, i, state):\n",
        "        # one evader assumption\n",
        "        evader_pos = self.world.evaders[0].position\n",
        "        if evader_pos[0] <0 or evader_pos[1]>19 or evader_pos[0] > 19 or evader_pos[1] < 0:\n",
        "            return -5, False, True\n",
        "        # one pursuer condition\n",
        "        elif any([np.array_equal(evader_pos, self.world.pursuers[i].position) for i in range(self.world.numPursuers)]):\n",
        "        # elif np.array_equal(evader_pos, self.world.pursuers[0].position):\n",
        "            return -10, True, False\n",
        "\n",
        "        dist = min([np.linalg.norm(evader_pos - self.world.pursuers[i].position) for i in range(self.world.numPursuers)])\n",
        "        dist = dist/(20*math.sqrt(2))\n",
        "        return dist, False, False\n",
        "\n",
        "\n",
        "    def reset(self):\n",
        "        self.iter = 0\n",
        "        self.world.reset_world()\n",
        "\n",
        "    def render(self):\n",
        "        step_size = 30\n",
        "        width = 20*step_size\n",
        "        height = 20*step_size\n",
        "\n",
        "        pg.init()\n",
        "        screen = pg.display.set_mode((width+5, height+5))\n",
        "\n",
        "        screen.fill((255,255,255))\n",
        "\n",
        "        for x in range(0, width+step_size, step_size):\n",
        "            pg.draw.line(screen, (0,0,0), (x, 0), (x, height))\n",
        "\n",
        "        for y in range(0, width+step_size, step_size):\n",
        "            pg.draw.line(screen, (0,0,0), (0, y), (width, y))\n",
        "            \n",
        "#         for i in range(20):\n",
        "#             for j in range(20):\n",
        "#                 if self.grid[i,j]==1:\n",
        "#                     pg.draw.rect(screen, pg.Color(\"black\"), (i*step_size, j*step_size, step_size, step_size))\n",
        "                    \n",
        "        red = (255,0,0)\n",
        "        blue = (0, 0, 128)\n",
        "\n",
        "        evader_x = (self.evader_p[0] + 0.5)*step_size\n",
        "        evader_y = (self.evader_p[1] + 0.5)*step_size\n",
        "\n",
        "        pursuer_x = (self.pursuer_p[0] + 0.5)*step_size\n",
        "        pursuer_y = (self.pursuer_p[1] + 0.5)*step_size    \n",
        "        r = 10\n",
        "        pg.draw.circle(screen, red, (int(evader_x),int(evader_y)), r)\n",
        "        pg.draw.circle(screen, blue, (int(pursuer_x),int(pursuer_y)), r)\n",
        "        \n",
        "        if self.iter < 10:\n",
        "            pg.image.save(screen, f\"Saved/screenshot00{self.iter}.png\")\n",
        "        elif self.iter < 100:\n",
        "            pg.image.save(screen, f\"Saved/screenshot0{self.iter}.png\")\n",
        "        else:\n",
        "            pg.image.save(screen, f\"Saved/screenshot{self.iter}.png\")        \n",
        "        \n",
        "        pg.display.update()\n",
        "        pg.time.delay(300)    \n",
        "\n",
        "    def close(self):\n",
        "        print(\"close\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVUSHGQ8y4uW"
      },
      "source": [
        "def play_game(env, epsilon_p, epsilon_e, copy_step):\n",
        "    cumulative_rewards_p = [0 for i in range(env.numPursuers)]\n",
        "    cumulative_rewards_e = [0 for i in range(env.numEvaders)]\n",
        "\n",
        "    iter = 0\n",
        "    done = False\n",
        "    violated_p = [False for i in range(env.numPursuers)]\n",
        "    violated_e = [False for i in range(env.numEvaders)]\n",
        "    env.reset()\n",
        "    losses_p = []\n",
        "    losses_e = []\n",
        "    num_violations_p = [0 for i in range(env.numPursuers)]\n",
        "    num_steps_p = [0 for i in range(env.numPursuers)]\n",
        "    # steps_to_violation_p = [0 for i in range(env.numPursuers)]\n",
        "    num_violations_e = [0 for i in range(env.numEvaders)]\n",
        "    # Assumption\n",
        "    success = 0\n",
        "    exceed_limit = 0\n",
        "    state = []\n",
        "    for agent in env.world.pursuers:\n",
        "        state.append(agent.position[0])\n",
        "        state.append(agent.position[1])\n",
        "\n",
        "    for agent in env.world.evaders:\n",
        "        state.append(agent.position[0])\n",
        "        state.append(agent.position[1])        \n",
        "\n",
        "    while not done:\n",
        "        actions_p =[]\n",
        "        NN_input_list = []\n",
        "        for i in range(env.numPursuers):\n",
        "            NN_input = env.world.get_NN_input(state, i, 'p')\n",
        "            NN_input_list.append(NN_input)\n",
        "            # one evader assumption\n",
        "            action = env.world.pursuers[i].TrainNet.get_action(NN_input, epsilon_p)\n",
        "            env.world.pursuers[i].lastAction = action\n",
        "            actions_p.append(action)\n",
        "        actions_e = []\n",
        "        for i in range(env.numEvaders):\n",
        "            # one evader assumption\n",
        "            action = env.world.evaders[i].TrainNet.get_action(state, epsilon_e)\n",
        "            actions_e.append(action)\n",
        "             \n",
        "        next_state, reward_p, reward_e, done, violated_p, violated_e = env.step(actions_p, actions_e, state)\n",
        "\n",
        "        if done:\n",
        "            success += 1\n",
        "\n",
        "        for i in range(env.numPursuers):\n",
        "            if violated_p[i]:\n",
        "                num_violations_p[i] += 1\n",
        "\n",
        "        for i in range(env.numEvaders):\n",
        "            if violated_e[i]:\n",
        "                num_violations_e[i] += 1\n",
        "\n",
        "        for i in range(env.numPursuers):\n",
        "            NN_input=NN_input_list[i]\n",
        "            next_NN_input = env.world.get_NN_input(next_state, i, 'p')\n",
        "            exp_p = {'s': NN_input, 'a': actions_p[i], 'r': reward_p[i], 's2': next_NN_input, 'done': done or violated_p[i]}        \n",
        "            env.world.pursuers[i].TrainNet.add_experience(exp_p)\n",
        "            loss_p = env.world.pursuers[i].TrainNet.train(env.world.pursuers[i].TargetNet)\n",
        "            env.world.pursuers[i].currLosses.append(float(loss_p))\n",
        "            num_steps_p[i] += 1\n",
        "            if env.world.pursuers[i].TrainNet.iter % copy_step == 0:\n",
        "                env.world.pursuers[i].TargetNet.copy_weights(env.world.pursuers[i].TrainNet)            \n",
        "\n",
        "        for i in range(env.numEvaders):\n",
        "            exp_e = {'s': state, 'a': actions_e[i], 'r': reward_e[i], 's2': next_state, 'done': done or violated_e[i]}        \n",
        "            env.world.evaders[i].TrainNet.add_experience(exp_e)\n",
        "            loss_e = env.world.evaders[i].TrainNet.train(env.world.evaders[i].TargetNet)\n",
        "            env.world.evaders[i].currLosses.append(float(loss_e))\n",
        "           \n",
        "            if env.world.evaders[i].TrainNet.iter % copy_step == 0:\n",
        "                env.world.evaders[i].TargetNet.copy_weights(env.world.evaders[i].TrainNet) \n",
        "\n",
        "\n",
        "        if num_steps_p[0]==150:\n",
        "            exceed_limit = 1\n",
        "            break\n",
        "            \n",
        "        state = next_state.copy()\n",
        "\n",
        "    return success, exceed_limit, num_violations_p, num_violations_e"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYxYmtTVXaxc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a211820a-dadf-42b5-f563-1d246038ff1a"
      },
      "source": [
        "env = MarlEnv()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Init\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDA01adsy4uv",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3567248-3a3a-48f9-dd7d-01fa92cb6c34"
      },
      "source": [
        "training_start = time.time()\n",
        "N = 1001\n",
        "# env = MarlEnv()\n",
        "copy_step = 500\n",
        "total_violations_p = np.empty(N)\n",
        "total_violations_e = np.empty(N)\n",
        "total_success = np.empty(N)\n",
        "total_exceed_limit = np.empty(N)\n",
        "epsilon_p = 0.9\n",
        "epsilon_e = 0.9\n",
        "# epsilon = 0.3\n",
        "decay = 0.998\n",
        "min_epsilon = 0.1\n",
        "for n in range(N):\n",
        "    epsilon_p = max(min_epsilon, epsilon_p * decay)\n",
        "    epsilon_e = max(min_epsilon, epsilon_e * decay)\n",
        "#     epsilon = max(min_epsilon, 0.99*(1-n/N))\n",
        "    success, exceed_limit, num_violations_p, num_violations_e = play_game(env, epsilon_p, epsilon_e, copy_step)\n",
        "    total_violations_p[n] = sum(num_violations_p)\n",
        "    total_violations_e[n] = sum(num_violations_e)\n",
        "    total_success[n] = success\n",
        "    total_exceed_limit[n] = exceed_limit\n",
        "    # total_steps_to_violations[n] = episode_steps_to_violation\n",
        "    if n>0 and n % 100 == 0:\n",
        "        print(\"episode:\", n, \"eps_p:\", epsilon_p,\"eps_e:\", epsilon_e)\n",
        "        for i in range(env.numPursuers):\n",
        "            avg_rewards_p = statistics.mean(env.world.pursuers[i].rewards[max(0, n - 100):(n + 1)])\n",
        "            print(\"avg reward_p\",i,\" (last 100):\", avg_rewards_p)\n",
        "        for i in range(env.numEvaders):\n",
        "            avg_rewards_e = statistics.mean(env.world.evaders[i].rewards[max(0, n - 100):(n + 1)])\n",
        "            print(\"avg reward_e\",i,\" (last 100):\", avg_rewards_e)\n",
        "        num_violations_p = total_violations_p[max(0, n - 100):(n + 1)].sum()\n",
        "        num_violations_e = total_violations_e[max(0, n - 100):(n + 1)].sum()\n",
        "        num_success = total_success[max(0, n - 100):(n + 1)].sum()\n",
        "        num_exceed_limit = total_exceed_limit[max(0, n - 100):(n + 1)].sum()\n",
        "        # avg_steps_to_violation = total_steps_to_violations[max(0, n - 100):(n + 1)].mean()\n",
        "        \n",
        "        print(\"Successes:\",  num_success,\"violations (pursuer):\", num_violations_p,\"violations (evader):\", num_violations_e, \"total times limit exceeded\", num_exceed_limit)\n",
        "        print(\"----------------------------------------------------------------------------------------\")\n",
        "# print(\"avg reward for last 100 episodes:\", avg_rewards)\n",
        "training_end = time.time()\n",
        "print(\"Training Time: \", training_end - training_start)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 339975 calls to <function MyModel.call at 0x7f0e24b0cd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 339978 calls to <function MyModel.call at 0x7f0e24b0c598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 266 calls to <function MyModel.call at 0x7f0e24b64620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 267 calls to <function MyModel.call at 0x7f0e24b0c1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:7 out of the last 268 calls to <function MyModel.call at 0x7f0e24b64d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:8 out of the last 269 calls to <function MyModel.call at 0x7f0e24b0c950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:9 out of the last 270 calls to <function MyModel.call at 0x7f0e24b0c598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:10 out of the last 271 calls to <function MyModel.call at 0x7f0e24b36158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:11 out of the last 272 calls to <function MyModel.call at 0x7f0e24b0cd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "episode: 100 eps_p: 0.735236703971146 eps_e: 0.735236703971146\n",
            "avg reward_p 0  (last 100): -21.831302697291857\n",
            "avg reward_p 1  (last 100): -22.25716133620907\n",
            "avg reward_p 2  (last 100): -20.95240603150042\n",
            "avg reward_p 3  (last 100): -20.464060307571547\n",
            "avg reward_e 0  (last 100): -18.68029719344508\n",
            "Successes: 91.0 violations (pursuer): 396.0 violations (evader): 389.0 total times limit exceeded 11.0\n",
            "----------------------------------------------------------------------------------------\n",
            "episode: 200 eps_p: 0.6018403594593125 eps_e: 0.6018403594593125\n",
            "avg reward_p 0  (last 100): -12.232254509747017\n",
            "avg reward_p 1  (last 100): -10.884581200259626\n",
            "avg reward_p 2  (last 100): -8.182694934665145\n",
            "avg reward_p 3  (last 100): -8.892110987809144\n",
            "avg reward_e 0  (last 100): -17.90659856745914\n",
            "Successes: 100.0 violations (pursuer): 151.0 violations (evader): 263.0 total times limit exceeded 1.0\n",
            "----------------------------------------------------------------------------------------\n",
            "episode: 300 eps_p: 0.49264653997514357 eps_e: 0.49264653997514357\n",
            "avg reward_p 0  (last 100): -10.255623115131343\n",
            "avg reward_p 1  (last 100): -9.32164039894759\n",
            "avg reward_p 2  (last 100): -8.14051265370612\n",
            "avg reward_p 3  (last 100): -8.810545769928503\n",
            "avg reward_e 0  (last 100): -17.16380038825272\n",
            "Successes: 101.0 violations (pursuer): 108.0 violations (evader): 242.0 total times limit exceeded 0.0\n",
            "----------------------------------------------------------------------------------------\n",
            "episode: 400 eps_p: 0.4032641040682629 eps_e: 0.4032641040682629\n",
            "avg reward_p 0  (last 100): -6.443632383751951\n",
            "avg reward_p 1  (last 100): -7.016471102101142\n",
            "avg reward_p 2  (last 100): -4.101030322018704\n",
            "avg reward_p 3  (last 100): -5.903356404359538\n",
            "avg reward_e 0  (last 100): -13.07301912465435\n",
            "Successes: 101.0 violations (pursuer): 53.0 violations (evader): 141.0 total times limit exceeded 0.0\n",
            "----------------------------------------------------------------------------------------\n",
            "episode: 500 eps_p: 0.3300986091126997 eps_e: 0.3300986091126997\n",
            "avg reward_p 0  (last 100): -5.439033137173913\n",
            "avg reward_p 1  (last 100): -4.658024034351382\n",
            "avg reward_p 2  (last 100): -3.359674295659828\n",
            "avg reward_p 3  (last 100): -4.65029911224513\n",
            "avg reward_e 0  (last 100): -11.740582087985109\n",
            "Successes: 101.0 violations (pursuer): 26.0 violations (evader): 104.0 total times limit exceeded 0.0\n",
            "----------------------------------------------------------------------------------------\n",
            "episode: 600 eps_p: 0.27020776369347727 eps_e: 0.27020776369347727\n",
            "avg reward_p 0  (last 100): -3.2405575968407967\n",
            "avg reward_p 1  (last 100): -4.6322300210385645\n",
            "avg reward_p 2  (last 100): -2.9362865903732214\n",
            "avg reward_p 3  (last 100): -3.22292846902099\n",
            "avg reward_e 0  (last 100): -11.612232603595766\n",
            "Successes: 101.0 violations (pursuer): 18.0 violations (evader): 97.0 total times limit exceeded 0.0\n",
            "----------------------------------------------------------------------------------------\n",
            "episode: 700 eps_p: 0.22118310572857544 eps_e: 0.22118310572857544\n",
            "avg reward_p 0  (last 100): -3.251912439109205\n",
            "avg reward_p 1  (last 100): -3.0447665773416532\n",
            "avg reward_p 2  (last 100): -4.413470017651041\n",
            "avg reward_p 3  (last 100): -4.058245357871776\n",
            "avg reward_e 0  (last 100): -11.263727699971035\n",
            "Successes: 101.0 violations (pursuer): 9.0 violations (evader): 94.0 total times limit exceeded 0.0\n",
            "----------------------------------------------------------------------------------------\n",
            "episode: 800 eps_p: 0.18105314810730258 eps_e: 0.18105314810730258\n",
            "avg reward_p 0  (last 100): -3.467517653389938\n",
            "avg reward_p 1  (last 100): -4.476041831497371\n",
            "avg reward_p 2  (last 100): -4.660451123121836\n",
            "avg reward_p 3  (last 100): -4.05468165562329\n",
            "avg reward_e 0  (last 100): -8.438012945251257\n",
            "Successes: 100.0 violations (pursuer): 11.0 violations (evader): 48.0 total times limit exceeded 1.0\n",
            "----------------------------------------------------------------------------------------\n",
            "episode: 900 eps_p: 0.14820409692497533 eps_e: 0.14820409692497533\n",
            "avg reward_p 0  (last 100): -8.052626025426456\n",
            "avg reward_p 1  (last 100): -7.803012146205848\n",
            "avg reward_p 2  (last 100): -6.67868875894106\n",
            "avg reward_p 3  (last 100): -6.084353783725966\n",
            "avg reward_e 0  (last 100): -7.722918048261063\n",
            "Successes: 93.0 violations (pursuer): 11.0 violations (evader): 91.0 total times limit exceeded 8.0\n",
            "----------------------------------------------------------------------------------------\n",
            "episode: 1000 eps_p: 0.12131495406161102 eps_e: 0.12131495406161102\n",
            "avg reward_p 0  (last 100): -11.83759540954049\n",
            "avg reward_p 1  (last 100): -11.597627931110564\n",
            "avg reward_p 2  (last 100): -7.267568678799806\n",
            "avg reward_p 3  (last 100): -6.787283281357549\n",
            "avg reward_e 0  (last 100): -8.838086866466407\n",
            "Successes: 93.0 violations (pursuer): 36.0 violations (evader): 131.0 total times limit exceeded 8.0\n",
            "----------------------------------------------------------------------------------------\n",
            "Training Time:  1735.0495624542236\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvnMGvZyocFp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad4ba875-d5e6-4c09-ab41-8ee0b6d7c809"
      },
      "source": [
        "print(env.world.evaders[0].TrainNet.iter)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20159\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KBVv2NgDS4E"
      },
      "source": [
        "#saving to csv\n",
        "import pandas as pd \n",
        "import statistics \n",
        "a = env.world.pursuers[0].rewards\n",
        "df = pd.DataFrame(a)\n",
        "df.to_csv('pursuer_rewards_a_MF1')\n",
        "b = env.world.pursuers[1].rewards\n",
        "df = pd.DataFrame(b)\n",
        "df.to_csv('pursuer_rewards_b_MF1')\n",
        "c = env.world.pursuers[2].rewards\n",
        "df = pd.DataFrame(c)\n",
        "df.to_csv('pursuer_rewards_c_MF1')\n",
        "d = env.world.pursuers[3].rewards\n",
        "df = pd.DataFrame(d)\n",
        "df.to_csv('pursuer_rewards_d_MF1')\n",
        "\n",
        "df = pd.DataFrame(env.world.evaders[0].losses)\n",
        "df.to_csv('evader_losses_MF1')\n",
        "df = pd.DataFrame(env.world.pursuers[1].losses)\n",
        "df.to_csv('pursuer_losses_MF1')\n",
        "\n",
        "avg_reward = [statistics.mean(k) for k in zip(a, b,c,d)]\n",
        "df = pd.DataFrame(avg_reward)  \n",
        "\n",
        "\n",
        "df.to_csv('avg_pursuer_training_reward_MF1.csv')\n",
        "df = pd.DataFrame([sum(total_violations_p[i:i+10]) for i in range(0, len(total_violations_p),10)])\n",
        "df.to_csv('total_pursuer_violations_MF1')\n",
        "df = pd.DataFrame([sum(total_violations_e[i:i+10]) for i in range(0, len(total_violations_e),10)])\n",
        "df.to_csv('total_evader_violations_MF1')\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHWjIQq4yHk5"
      },
      "source": [
        "#Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QO3Yx6_RyKyw"
      },
      "source": [
        "def play_game_test(env):\n",
        "    done = False\n",
        "    violated_p = [False for i in range(env.numPursuers)]\n",
        "    violated_e = [False for i in range(env.numEvaders)]\n",
        "    env.reset()\n",
        "    losses_p = []\n",
        "    losses_e = []\n",
        "    num_violations_p = [0 for i in range(env.numPursuers)]\n",
        "    num_steps_p = [0 for i in range(env.numPursuers)]\n",
        "    # steps_to_violation_p = [0 for i in range(env.numPursuers)]\n",
        "    num_violations_e = [0 for i in range(env.numEvaders)]\n",
        "    # Assumption\n",
        "    success = 0\n",
        "    exceed_limit = 0\n",
        "    # env.render()\n",
        "    state = []\n",
        "    for agent in env.world.pursuers:\n",
        "        state.append(agent.position[0])\n",
        "        state.append(agent.position[1])\n",
        "\n",
        "    for agent in env.world.evaders:\n",
        "        state.append(agent.position[0])\n",
        "        state.append(agent.position[1])        \n",
        "\n",
        "    while not done:\n",
        "        actions_p =[]\n",
        "        for i in range(env.numPursuers):\n",
        "            NN_input = env.world.get_NN_input(state, i, 'p')\n",
        "            # one evader assumption\n",
        "            action = env.world.pursuers[i].TrainNet.get_action(NN_input, 0)\n",
        "            env.world.pursuers[i].lastAction = action \n",
        "            actions_p.append(action)           \n",
        "        actions_e = []\n",
        "        for i in range(env.numEvaders):\n",
        "            # one evader assumption\n",
        "            action = env.world.evaders[i].TrainNet.get_action(state, 0)\n",
        "            actions_e.append(action)\n",
        "             \n",
        "        next_state, reward_p, reward_e, done, violated_p, violated_e = env.step(actions_p, actions_e, state)\n",
        "        # env.render()\n",
        "        \n",
        "        if done:\n",
        "            success += 1\n",
        "\n",
        "        for i in range(env.numPursuers):\n",
        "            if violated_p[i]:\n",
        "                num_violations_p[i] += 1\n",
        "\n",
        "        for i in range(env.numEvaders):\n",
        "            if violated_e[i]:\n",
        "                num_violations_e[i] += 1\n",
        "\n",
        "        num_steps_p[0] += 1\n",
        "        if num_steps_p[0]==150:\n",
        "            exceed_limit = 1\n",
        "            break\n",
        "\n",
        "        state = next_state.copy()\n",
        "\n",
        "    return success, exceed_limit, num_violations_p, num_violations_e, num_steps_p[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNeZgm8VypC7"
      },
      "source": [
        "test_episodes = 500\n",
        "steps_taken = []\n",
        "for i in range(test_episodes):\n",
        "    success, exceed_limit, num_violations_p, num_violations_e, time_steps = play_game_test(env)\n",
        "    # print(success)\n",
        "    # print(exceed_limit)\n",
        "    if success:\n",
        "        steps_taken.append(time_steps)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}